{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d784d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.player_symbol = 1\n",
    "    \n",
    "    def get_hash(self):\n",
    "        return str(self.board.reshape(BOARD_SIZE))\n",
    "\n",
    "    def winner(self):\n",
    "        for i in range(3):\n",
    "            if sum(self.board[i, :]) == 3 or sum(self.board[:, i]) == 3: return 1\n",
    "            if sum(self.board[i, :]) == -3 or sum(self.board[:, i]) == -3: return -1\n",
    "        diag1 = sum([self.board[i, i] for i in range(3)])\n",
    "        diag2 = sum([self.board[i, 2 - i] for i in range(3)])\n",
    "        if diag1 == 3 or diag2 == 3: return 1\n",
    "        if diag1 == -3 or diag2 == -3: return -1\n",
    "        if len(self.available_positions()) == 0: return 0\n",
    "        return None\n",
    "\n",
    "    def available_positions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def update_state(self, position):\n",
    "        self.board[position] = self.player_symbol\n",
    "        self.player_symbol = -1 if self.player_symbol == 1 else 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.player_symbol = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde9bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, exp_rate=0.3):\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {} \n",
    "        self.states = []\n",
    "\n",
    "    def choose_action(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            return positions[np.random.choice(len(positions))]\n",
    "        \n",
    "        value_max = -999\n",
    "        action = positions[0]\n",
    "        for p in positions:\n",
    "            nb = current_board.copy()\n",
    "            nb[p] = symbol\n",
    "            h = str(nb.reshape(9))\n",
    "            value = self.states_value.get(h, 0)\n",
    "            if value >= value_max:\n",
    "                value_max, action = value, p\n",
    "        return action\n",
    "\n",
    "    def feed_reward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None: self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d39c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(eps, total_runs=100, steps=1000):\n",
    "    # This will store the win result (1 or 0) for every step, averaged across all runs\n",
    "    aggregated_results = np.zeros(steps)\n",
    "    \n",
    "    print(f\"Running {total_runs} independent experiments for epsilon={eps}...\")\n",
    "    for r in tqdm(range(total_runs)):\n",
    "        p1 = Agent(exp_rate=eps)\n",
    "        p2 = Agent(exp_rate=0.1) # Standard opponent\n",
    "        env = TicTacToe()\n",
    "        \n",
    "        for s in range(steps):\n",
    "            winner = None\n",
    "            while winner is None:\n",
    "                # P1 Turn\n",
    "                act1 = p1.choose_action(env.available_positions(), env.board, 1)\n",
    "                env.update_state(act1)\n",
    "                p1.states.append(env.get_hash())\n",
    "                winner = env.winner()\n",
    "                if winner is not None:\n",
    "                    p1.feed_reward(1 if winner == 1 else 0.1 if winner == 0 else 0)\n",
    "                    break\n",
    "                \n",
    "                # P2 Turn\n",
    "                act2 = p2.choose_action(env.available_positions(), env.board, -1)\n",
    "                env.update_state(act2)\n",
    "                winner = env.winner()\n",
    "                if winner is not None:\n",
    "                    p1.feed_reward(0 if winner == -1 else 0.1 if winner == 0 else 1)\n",
    "            \n",
    "            if winner == 1:\n",
    "                aggregated_results[s] += 1\n",
    "            \n",
    "            env.reset()\n",
    "            p1.states = []\n",
    "            \n",
    "    return aggregated_results / total_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\Dushyant Yadav\\AppData\\Local\\Temp\\ipykernel_10504\\3985273175.py:10: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(res_greedy, label='$\\epsilon = 0$ (greedy)', color='gray')\n",
      "C:\\Users\\Dushyant Yadav\\AppData\\Local\\Temp\\ipykernel_10504\\3985273175.py:11: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(res_eps_01, label='$\\epsilon = 0.1$', color='green')\n",
      "C:\\Users\\Dushyant Yadav\\AppData\\Local\\Temp\\ipykernel_10504\\3985273175.py:12: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot(res_eps_03, label='$\\epsilon = 0.3$', color='red')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 independent experiments for epsilon=0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [37:49<00:00, 22.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 independent experiments for epsilon=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [15:41<43:47, 39.21s/it]"
     ]
    }
   ],
   "source": [
    "# Settings: 100 independent runs, 5000 steps each\n",
    "RUNS = 100 \n",
    "STEPS = 5000\n",
    "\n",
    "res_greedy = run_experiment(0.0, total_runs=RUNS, steps=STEPS)\n",
    "res_eps_01 = run_experiment(0.1, total_runs=RUNS, steps=STEPS)\n",
    "res_eps_03 = run_experiment(0.3, total_runs=RUNS, steps=STEPS)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(res_greedy, label='$\\epsilon = 0$ (greedy)', color='gray')\n",
    "plt.plot(res_eps_01, label='$\\epsilon = 0.1$', color='green')\n",
    "plt.plot(res_eps_03, label='$\\epsilon = 0.3$', color='red')\n",
    "\n",
    "plt.xlabel('Steps (Episodes)')\n",
    "plt.ylabel('Average Reward (Win Probability)')\n",
    "plt.title('10-Armed Testbed Logic applied to Tic-Tac-Toe (Ensemble Average)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
